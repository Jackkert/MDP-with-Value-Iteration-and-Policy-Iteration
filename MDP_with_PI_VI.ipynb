{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements value iteration and policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process - MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MDP, there is an agent. The agent choose an action $a_{t}$ at time $t$ and as a consequence, the environment changes.\n",
    "Here the evniorment is world around the agent. After taking the action, the environment state changes to $s_{t+1}$.\n",
    "A reward might be emitted associated with what just happened and then this process repeats. ![](nb_images/mdp.png)\n",
    "\n",
    "So, there is a feedback cycle in that the next action you take, the next decision you make is in a situation that's the consiquence of what you did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium.spaces as spaces\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "← ↓ → ↑\n"
     ]
    }
   ],
   "source": [
    "# action mapping for display the final result\n",
    "action_mapping = {\n",
    "    3: '\\u2191', # UP\n",
    "    2: '\\u2192', # RIGHT\n",
    "    1: '\\u2193', # DOWN\n",
    "    0: '\\u2190' # LEFT\n",
    "}\n",
    "print(' '.join([action_mapping[i] for i in range(4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup GYM Env for playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will take a GYM environment and plays number of games according to given policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy, random = False, categorial = False):\n",
    "    \"\"\"\n",
    "    This fucntion plays the given number of episodes given by following a policy or sample randomly from action_space.\n",
    "    \n",
    "    Parameters:\n",
    "        environment: openAI GYM object\n",
    "        n_episodes: number of episodes to run\n",
    "        policy: Policy to follow while playing an episode\n",
    "        random: Flag for taking random actions. if True no policy would be followed and action will be taken randomly\n",
    "        \n",
    "    Return:\n",
    "        wins: Total number of wins playing n_episodes\n",
    "        total_reward: Total reward of n_episodes\n",
    "        avg_reward: Average reward of n_episodes\n",
    "    \n",
    "    \"\"\"\n",
    "    # intialize wins and total reward\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # loop over number of episodes to play\n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        # flag to check if the game is finished\n",
    "        terminated = False\n",
    "        \n",
    "        # reset the environment every time when playing a new episode\n",
    "        state = environment.reset()[0]\n",
    "\n",
    "        while not terminated:\n",
    "            \n",
    "            # check if the random flag is not true then follow the given policy other wise take random action\n",
    "            if random:\n",
    "                action = environment.action_space.sample()\n",
    "            else:\n",
    "                if categorial:\n",
    "                    action = np.random.choice(len(policy[state]), p=policy[state])\n",
    "                else:\n",
    "                    action = policy[state] \n",
    "\n",
    "            # take the next step\n",
    "            next_state, reward,  terminated, info = environment.step(action)[:-1]\n",
    "            \n",
    "            environment.render()\n",
    "            \n",
    "            # accumalate total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # change the state\n",
    "            state = next_state\n",
    "            \n",
    "            # if game is over with positive reward then add 1.0 in wins\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "                \n",
    "    # calculate average reward\n",
    "    average_reward = total_reward / n_episodes\n",
    "    \n",
    "    return wins, total_reward, average_reward\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solve for Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nb_images/value_iter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_step_lookahead(env, state, V , discount_factor = 0.99):\n",
    "    \"\"\"\n",
    "    Helper function to  calculate state-value function\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object\n",
    "        state: state to consider\n",
    "        V: Estimated Value for each state. Vector of length nS\n",
    "        discount_factor: MDP discount factor\n",
    "        \n",
    "    Return:\n",
    "        action_values: Expected value of each action in a state. Vector of length nA\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vector of action values\n",
    "    action_values = np.zeros(env.action_space.n)\n",
    "    \n",
    "    # loop over the actions we can take in an environment \n",
    "    for action in range(env.action_space.n):\n",
    "        # loop over the P_sa distribution.\n",
    "        for probablity, next_state, reward, info in env.P[state][action]:\n",
    "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
    "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(env, policy, V, discount_factor):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to update a given policy based on given value function.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        policy: policy to update.\n",
    "        V: Estimated Value for each state. Vector of length nS.\n",
    "        discount_factor: MDP discount factor.\n",
    "    Return:\n",
    "        policy: Updated policy based on the given state-Value function 'V'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        # for a given state compute state-action value.\n",
    "        action_values = one_step_lookahead(env, state, V, discount_factor)\n",
    "        \n",
    "        # choose the action which maximizez the state-action value.\n",
    "        policy[state] =  np.argmax(action_values)\n",
    "        \n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "    Algorithm to solve MPD.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        discount_factor: MDP discount factor.\n",
    "        max_iteration: Maximum No.  of iterations to run.\n",
    "        \n",
    "    Return:\n",
    "        V: Optimal state-Value function. Vector of lenth nS.\n",
    "        optimal_policy: Optimal policy. Vector of length nS.\n",
    "    \n",
    "    \"\"\"\n",
    "    # intialize value fucntion\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # iterate over max_iterations\n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        #  keep track of change with previous value function\n",
    "        prev_v = np.copy(V) \n",
    "    \n",
    "        # loop over all states\n",
    "        for state in range(env.observation_space.n):\n",
    "            \n",
    "            # Asynchronously update the state-action value\n",
    "            #action_values = one_step_lookahead(env, state, V, discount_factor)\n",
    "            \n",
    "            # Synchronously update the state-action value\n",
    "            action_values = one_step_lookahead(env, state, prev_v, discount_factor)\n",
    "            \n",
    "            # select best action to perform based on highest state-action value\n",
    "            best_action_value = np.max(action_values)\n",
    "            \n",
    "            # update the current state-value fucntion\n",
    "            V[state] =  best_action_value\n",
    "            \n",
    "        # if policy not changed over 10 iterations it converged.\n",
    "        if i % 10 == 0:\n",
    "            # if values of 'V' not changing after one iteration\n",
    "            if (np.all(np.isclose(V, prev_v))):\n",
    "                print('Value converged at iteration %d' %(i+1))\n",
    "                break\n",
    "\n",
    "    # intialize optimal policy\n",
    "    optimal_policy = np.zeros(env.observation_space.n, dtype = 'int8')\n",
    "    \n",
    "    # update the optimal polciy according to optimal value function 'V'\n",
    "    optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
    "    \n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Algorithim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value converged at iteration 341\n",
      "Time to converge:  1.27e+02 ms\n",
      "Optimal Value function: \n",
      "[[0.78538826 0.77836049 0.77368481 0.7713498 ]\n",
      " [0.78775777 0.         0.50562724 0.        ]\n",
      " [0.79250312 0.79963699 0.74472318 0.        ]\n",
      " [0.         0.86409247 0.93114742 0.        ]]\n",
      "Final Policy: \n",
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "← ↑ ↑ ↑ ← ← ← ← ↑ ↓ ← ← ← → ↓ ←\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackkert/anaconda3/envs/DRL_project/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "tic = time.time()\n",
    "opt_V, opt_Policy = value_iteration(environment.env, max_iteration = 1000)\n",
    "toc = time.time()\n",
    "elapsed_time = (toc - tic) * 1000\n",
    "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
    "print('Optimal Value function: ')\n",
    "print(opt_V.reshape((4, 4)))\n",
    "print('Final Policy: ')\n",
    "print(opt_Policy)\n",
    "print(' '.join([action_mapping[int(action)] for action in opt_Policy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackkert/anaconda3/envs/DRL_project/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:328: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "n_episode = 10\n",
    "wins, total_reward, avg_reward = play_episodes(environment, n_episode, opt_Policy, random = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wins with value iteration: 9\n",
      "Average rewards with value iteration: 0.9\n"
     ]
    }
   ],
   "source": [
    "print(f'Total wins with value iteration: {wins}')\n",
    "print(f\"Average rewards with value iteration: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solve for Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nb_images/policy_iter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(env, policy, V, discount_factor):\n",
    "    \"\"\"\n",
    "    Helper function to evaluate a policy.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        policy: policy to evaluate.\n",
    "        V: Estimated Value for each state. Vector of length nS.\n",
    "        discount_factor: MDP discount factor.\n",
    "    Return:\n",
    "        policy_value: Estimated value of each state following a given policy and state-value 'V'. \n",
    "        \n",
    "    \"\"\"\n",
    "    policy_value = np.zeros(env.observation_space.n)\n",
    "    for state in range(env.observation_space.n):\n",
    "        for action, action_prob in enumerate(policy[state]): # evaluate policy value over all actions\n",
    "            for probablity, next_state, reward, info in env.P[state][action]:\n",
    "                policy_value[state] += action_prob * probablity * (reward + (discount_factor * V[next_state]))\n",
    "    \n",
    "    return policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.22358262,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing policy_eval for categorial policy\n",
    "env_test = gym.make('FrozenLake-v1')\n",
    "a = np.random.randint(0, 1000, (env_test.observation_space.n, env_test.action_space.n))\n",
    "policy_test = a/a.sum(axis = 1, keepdims = True)\n",
    "V_test = np.zeros(env_test.observation_space.n)\n",
    "discount_factor_test = 0.9999\n",
    "\n",
    "policy_eval(env_test, policy_test, V_test, discount_factor_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead_categorial(env, state, V , discount_factor = 0.99):\n",
    "    \"\"\"\n",
    "    Helper function to  calculate state-value function\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object\n",
    "        state: state to consider\n",
    "        V: Estimated Value for each state. Vector of length nS\n",
    "        discount_factor: MDP discount factor\n",
    "        \n",
    "    Return:\n",
    "        action_values: Expected value of each action in a state. Vector of length nA\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vector of action values\n",
    "    action_values = np.zeros(env.action_distributions.shape[0]) \n",
    "    # loop over the actions we can take in an environment \n",
    "    for action_dist_id, action_distribution in enumerate(env.action_distributions): \n",
    "        # loop over the P_sa distribution.\n",
    "        for action, action_prob in enumerate(action_distribution):\n",
    "            for probablity, next_state, reward, info in env.P[state][action]:\n",
    "                 #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
    "                action_values[action_dist_id] += action_prob * probablity * (reward + (discount_factor * V[next_state]))\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_categorial(env, policy, V, discount_factor):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to update a given policy based on given value function.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        policy: categorial policy to update.\n",
    "        V: Estimated Value for each state. Vector of length nS.\n",
    "        discount_factor: MDP discount factor.\n",
    "    Return:\n",
    "        policy: Updated policy based on the given state-Value function 'V'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        # for a given state compute state-action value.\n",
    "        action_values = one_step_lookahead_categorial(env, state, V, discount_factor)\n",
    "        # choose the action which maximizez the state-action value.\n",
    "        id_max_action_value = np.argmax(action_values)\n",
    "        policy[state] = env.action_distributions[id_max_action_value]\n",
    "        \n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "    Algorithm to solve MPD.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        discount_factor: MDP discount factor.\n",
    "        max_iteration: Maximum No.  of iterations to run.\n",
    "        \n",
    "    Return:\n",
    "        V: Optimal state-Value function. Vector of lenth nS.\n",
    "        new_policy: Optimal policy. Vector of length nS.\n",
    "    \n",
    "    \"\"\"\n",
    "    # intialize the state-Value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    # create N random action distributions in tabular fashion:\n",
    "    N_dists = 10000\n",
    "    random_actions = np.random.randint(0, 1000, (N_dists, env.action_space.n))\n",
    "    env.action_distributions = random_actions/random_actions.sum(axis = 1, keepdims = True)\n",
    "    # intialize a random implicit categorial policy\n",
    "    dists_idx = np.arange(N_dists)\n",
    "    np.random.shuffle(dists_idx)\n",
    "    policy = env.action_distributions[dists_idx[:env.observation_space.n]]\n",
    "    policy_prev = np.copy(policy)\n",
    "    for i in range(max_iteration):\n",
    "        # evaluate given policy\n",
    "        V = policy_eval(env, policy, V, discount_factor)\n",
    "        \n",
    "        # improve policy\n",
    "        policy = update_policy_categorial(env, policy, V, discount_factor)\n",
    "        \n",
    "        # if policy not changed over 10 iterations it converged.\n",
    "        if i % 10 == 0:\n",
    "            if (np.all(np.equal(policy, policy_prev))):\n",
    "                print('policy converged at iteration %d' %(i+1))\n",
    "                break\n",
    "            policy_prev = np.copy(policy)\n",
    "            \n",
    "\n",
    "            \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy converged at iteration 41\n",
      "Time to converge:  1.28e+05 ms\n",
      "Optimal Value function: \n",
      "[[0.25857573 0.22056746 0.21555792 0.18758814]\n",
      " [0.2731765  0.         0.22067219 0.        ]\n",
      " [0.32858422 0.43530956 0.45475947 0.        ]\n",
      " [0.         0.5726475  0.76849063 0.        ]]\n",
      "Final Policy: \n",
      "[[0.88762984 0.01605288 0.06515581 0.03116147]\n",
      " [0.08144192 0.03204272 0.         0.88651535]\n",
      " [0.88762984 0.01605288 0.06515581 0.03116147]\n",
      " [0.08144192 0.03204272 0.         0.88651535]\n",
      " [0.88762984 0.01605288 0.06515581 0.03116147]\n",
      " [0.10220441 0.29579158 0.36873747 0.23326653]\n",
      " [0.40139616 0.01628854 0.57998837 0.00232693]\n",
      " [0.10220441 0.29579158 0.36873747 0.23326653]\n",
      " [0.08144192 0.03204272 0.         0.88651535]\n",
      " [0.0523416  0.88888889 0.0523416  0.00642792]\n",
      " [0.88762984 0.01605288 0.06515581 0.03116147]\n",
      " [0.10220441 0.29579158 0.36873747 0.23326653]\n",
      " [0.10220441 0.29579158 0.36873747 0.23326653]\n",
      " [0.0254842  0.03975535 0.90519878 0.02956167]\n",
      " [0.01088032 0.87240356 0.0504451  0.06627102]\n",
      " [0.10220441 0.29579158 0.36873747 0.23326653]]\n",
      "Most likely actions taken by policy: \n",
      "← ↑ ← ↑ ← → → → ↑ ↓ ← → → → ↓ →\n"
     ]
    }
   ],
   "source": [
    "environment2 = gym.make('FrozenLake-v1') #, render_mode = \"human\")\n",
    "tic = time.time()\n",
    "opt_V2, opt_policy2 = policy_iteration(environment2.env, discount_factor = 0.999, max_iteration = 10000)\n",
    "toc = time.time()\n",
    "elapsed_time = (toc - tic) * 1000\n",
    "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
    "print('Optimal Value function: ')\n",
    "print(opt_V2.reshape((4, 4)))\n",
    "print('Final Policy: ')\n",
    "print(opt_policy2)\n",
    "# Display the most likely policy in discrete action\n",
    "print(\"Most likely actions taken by policy: \")\n",
    "print(' '.join([action_mapping[(np.argmax(action))] for action in opt_policy2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 10\n",
    "wins, total_reward, avg_reward = play_episodes(environment2, n_episode, opt_policy2, random = False, categorial = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wins with Policy iteration: 3\n",
      "Average rewards with Policy iteration: 0.3\n"
     ]
    }
   ],
   "source": [
    "print(f'Total wins with Policy iteration: {wins}')\n",
    "print(f\"Average rewards with Policy iteration: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration converge faster but takes more computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
