{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements value iteration and policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process - MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MDP, there is an agent. The agent choose an action $a_{t}$ at time $t$ and as a consequence, the environment changes.\n",
    "Here the evniorment is world around the agent. After taking the action, the environment state changes to $s_{t+1}$.\n",
    "A reward might be emitted associated with what just happened and then this process repeats. ![](nb_images/mdp.png)\n",
    "\n",
    "So, there is a feedback cycle in that the next action you take, the next decision you make is in a situation that's the consiquence of what you did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium.spaces as spaces\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "← ↓ → ↑\n"
     ]
    }
   ],
   "source": [
    "# action mapping for display the final result\n",
    "action_mapping = {\n",
    "    3: '\\u2191', # UP\n",
    "    2: '\\u2192', # RIGHT\n",
    "    1: '\\u2193', # DOWN\n",
    "    0: '\\u2190' # LEFT\n",
    "}\n",
    "print(' '.join([action_mapping[i] for i in range(4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup GYM Env for playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will take a GYM environment and plays number of games according to given policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy, random = False, categorial = False):\n",
    "    \"\"\"\n",
    "    This fucntion plays the given number of episodes given by following a policy or sample randomly from action_space.\n",
    "    \n",
    "    Parameters:\n",
    "        environment: openAI GYM object\n",
    "        n_episodes: number of episodes to run\n",
    "        policy: Policy to follow while playing an episode\n",
    "        random: Flag for taking random actions. if True no policy would be followed and action will be taken randomly\n",
    "        \n",
    "    Return:\n",
    "        wins: Total number of wins playing n_episodes\n",
    "        total_reward: Total reward of n_episodes\n",
    "        avg_reward: Average reward of n_episodes\n",
    "    \n",
    "    \"\"\"\n",
    "    # intialize wins and total reward\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # loop over number of episodes to play\n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        # flag to check if the game is finished\n",
    "        terminated = False\n",
    "        \n",
    "        # reset the environment every time when playing a new episode\n",
    "        state = environment.reset()[0]\n",
    "\n",
    "        while not terminated:\n",
    "            \n",
    "            # check if the random flag is not true then follow the given policy other wise take random action\n",
    "            if random:\n",
    "                action = environment.action_space.sample()\n",
    "            else:\n",
    "                if categorial:\n",
    "                    action = np.random.choice(len(policy[state]), p=policy[state])\n",
    "                else:\n",
    "                    action = policy[state] \n",
    "\n",
    "            # take the next step\n",
    "            next_state, reward,  terminated, info = environment.step(action)[:-1]\n",
    "            \n",
    "            environment.render()\n",
    "            \n",
    "            # accumalate total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # change the state\n",
    "            state = next_state\n",
    "            \n",
    "            # if game is over with positive reward then add 1.0 in wins\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "                \n",
    "    # calculate average reward\n",
    "    average_reward = total_reward / n_episodes\n",
    "    \n",
    "    return wins, total_reward, average_reward\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solve for Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nb_images/value_iter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_step_lookahead(env, state, V , discount_factor = 0.99):\n",
    "    \"\"\"\n",
    "    Helper function to  calculate state-value function\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object\n",
    "        state: state to consider\n",
    "        V: Estimated Value for each state. Vector of length nS\n",
    "        discount_factor: MDP discount factor\n",
    "        \n",
    "    Return:\n",
    "        action_values: Expected value of each action in a state. Vector of length nA\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vector of action values\n",
    "    action_values = np.zeros(env.action_space.n)\n",
    "    \n",
    "    # loop over the actions we can take in an environment \n",
    "    for action in range(env.action_space.n):\n",
    "        # loop over the P_sa distribution.\n",
    "        for probablity, next_state, reward, info in env.P[state][action]:\n",
    "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
    "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(env, policy, V, discount_factor):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to update a given policy based on given value function.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        policy: policy to update.\n",
    "        V: Estimated Value for each state. Vector of length nS.\n",
    "        discount_factor: MDP discount factor.\n",
    "    Return:\n",
    "        policy: Updated policy based on the given state-Value function 'V'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        # for a given state compute state-action value.\n",
    "        action_values = one_step_lookahead(env, state, V, discount_factor)\n",
    "        \n",
    "        # choose the action which maximizez the state-action value.\n",
    "        policy[state] =  np.argmax(action_values)\n",
    "        \n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "    Algorithm to solve MPD.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        discount_factor: MDP discount factor.\n",
    "        max_iteration: Maximum No.  of iterations to run.\n",
    "        \n",
    "    Return:\n",
    "        V: Optimal state-Value function. Vector of lenth nS.\n",
    "        optimal_policy: Optimal policy. Vector of length nS.\n",
    "    \n",
    "    \"\"\"\n",
    "    # intialize value fucntion\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # iterate over max_iterations\n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        #  keep track of change with previous value function\n",
    "        prev_v = np.copy(V) \n",
    "    \n",
    "        # loop over all states\n",
    "        for state in range(env.observation_space.n):\n",
    "            \n",
    "            # Asynchronously update the state-action value\n",
    "            #action_values = one_step_lookahead(env, state, V, discount_factor)\n",
    "            \n",
    "            # Synchronously update the state-action value\n",
    "            action_values = one_step_lookahead(env, state, prev_v, discount_factor)\n",
    "            \n",
    "            # select best action to perform based on highest state-action value\n",
    "            best_action_value = np.max(action_values)\n",
    "            \n",
    "            # update the current state-value fucntion\n",
    "            V[state] =  best_action_value\n",
    "            \n",
    "        # if policy not changed over 10 iterations it converged.\n",
    "        if i % 10 == 0:\n",
    "            # if values of 'V' not changing after one iteration\n",
    "            if (np.all(np.isclose(V, prev_v))):\n",
    "                print('Value converged at iteration %d' %(i+1))\n",
    "                break\n",
    "\n",
    "    # intialize optimal policy\n",
    "    optimal_policy = np.zeros(env.observation_space.n, dtype = 'int8')\n",
    "    \n",
    "    # update the optimal polciy according to optimal value function 'V'\n",
    "    optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
    "    \n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Algorithim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value converged at iteration 341\n",
      "Time to converge:  2.77e+02 ms\n",
      "Optimal Value function: \n",
      "[[0.78538826 0.77836049 0.77368481 0.7713498 ]\n",
      " [0.78775777 0.         0.50562724 0.        ]\n",
      " [0.79250312 0.79963699 0.74472318 0.        ]\n",
      " [0.         0.86409247 0.93114742 0.        ]]\n",
      "Final Policy: \n",
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "← ↑ ↑ ↑ ← ← ← ← ↑ ↓ ← ← ← → ↓ ←\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackkert/anaconda3/envs/ics/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "tic = time.time()\n",
    "opt_V, opt_Policy = value_iteration(environment.env, max_iteration = 1000)\n",
    "toc = time.time()\n",
    "elapsed_time = (toc - tic) * 1000\n",
    "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
    "print('Optimal Value function: ')\n",
    "print(opt_V.reshape((4, 4)))\n",
    "print('Final Policy: ')\n",
    "print(opt_Policy)\n",
    "print(' '.join([action_mapping[int(action)] for action in opt_Policy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackkert/anaconda3/envs/ics/lib/python3.11/site-packages/gymnasium/envs/toy_text/frozen_lake.py:328: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "n_episode = 10\n",
    "wins, total_reward, avg_reward = play_episodes(environment, n_episode, opt_Policy, random = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wins with value iteration: 8\n",
      "Average rewards with value iteration: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(f'Total wins with value iteration: {wins}')\n",
    "print(f\"Average rewards with value iteration: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solve for Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(env, policy, Q, discount_factor):\n",
    "    \"\"\"\n",
    "    Helper function to evaluate a policy.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        policy: policy to evaluate.\n",
    "        Q: Estimated Value for each state-action pair. Table of shape N x A\n",
    "        discount_factor: MDP discount factor.\n",
    "    Return:\n",
    "        policy_value: Estimated value of each state-action pair following a given policy and state-action value 'Q'. \n",
    "        \n",
    "    \"\"\"\n",
    "    policy_value = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(env.observation_space.n):\n",
    "        for action, action_prob in enumerate(policy[state]): # evaluate policy value over all actions\n",
    "            for probablity, next_state, reward, info in env.P[state][action]:\n",
    "                policy_value[state][action] += probablity * (reward + (discount_factor * np.max(Q[next_state])))\n",
    "    \n",
    "    return policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead_categorial(env, state, Q , discount_factor = 0.99):\n",
    "    \"\"\"\n",
    "    Helper function to  calculate state-value function\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object\n",
    "        state: state to consider\n",
    "        Q: Estimated Value for each state-action pair. Table of shape N x A\n",
    "        discount_factor: MDP discount factor\n",
    "        \n",
    "    Return:\n",
    "        action_values: Expected value of each action in a state. Vector of length nA\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vector of action values\n",
    "    action_values = np.zeros(env.action_space.n) \n",
    "    # loop over the actions we can take in an environment \n",
    "    for action in range(env.action_space.n):\n",
    "        # loop over the P_sa distribution.\n",
    "        for probablity, next_state, reward, info in env.P[state][action]:\n",
    "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
    "            action_values[action] += probablity * (reward + (discount_factor * np.max(Q[next_state])))\n",
    "            \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_regularized_softmax(policy, temperature, Q_value):\n",
    "    \"\"\"\n",
    "    Helper function for the policy-regularized softmax function.\n",
    "\n",
    "    Arguments:\n",
    "        policy: current evaluated policy\n",
    "        temperature: temperature hyperparameter \n",
    "        Q_value: current Q value function\n",
    "    Return:\n",
    "        action_dist: Improved action probability distribution for the new policy.\n",
    "    \"\"\"\n",
    "    # convert to torch tensor\n",
    "    policy = torch.from_numpy(policy)\n",
    "    action_dist = torch.nn.Softmax(torch.log(policy) + temperature * Q_value)\n",
    "    return action_dist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update policy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_categorial(env, policy, Q, discount_factor, improvement_operator = np.argmax):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to update a given policy based on given value function.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        policy: categorial policy to update.\n",
    "        Q: Estimated Value for each state-action pair. Table of shape N x S.\n",
    "        discount_factor: MDP discount factor.\n",
    "    Return:\n",
    "        policy: Updated policy based on the given state-action value function 'Q'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        # for a given state compute state-action value.\n",
    "        action_values = one_step_lookahead_categorial(env, state, Q, discount_factor)\n",
    "        # choose the action which maximizez the state-action value.\n",
    "        max_action_value = improvement_operator(action_values)\n",
    "        one_hot_policy = np.zeros(env.action_space.n)\n",
    "        one_hot_policy[max_action_value] = 1\n",
    "        policy[state] = one_hot_policy\n",
    "        \n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_regularized_softmax(env, policy, Q, temperature):\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        policy[state] = policy_regularized_softmax(policy, temperature, Q)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "    Algorithm to solve MPD.\n",
    "    \n",
    "    Arguments:\n",
    "        env: openAI GYM environment object.\n",
    "        discount_factor: MDP discount factor.\n",
    "        max_iteration: Maximum No.  of iterations to run.\n",
    "        \n",
    "    Return:\n",
    "        Q: Optimal state-action value function. Table of N x A\n",
    "        new_policy: Optimal policy. Table of N x A\n",
    "    \n",
    "    \"\"\"\n",
    "    # intialize the state-Value function\n",
    "    Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    \n",
    "    # intialize a random implicit categorial policy\n",
    "    policy = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    policy[:,0] = 1\n",
    "    for row in policy:\n",
    "        np.random.shuffle(row)\n",
    "\n",
    "    policy_prev = np.copy(policy)\n",
    "    temperature = 1.0\n",
    "    for i in range(max_iteration):\n",
    "        # evaluate given policy\n",
    "        Q = policy_eval(env, policy, Q, discount_factor)\n",
    "        # improve policy\n",
    "        policy = update_policy_categorial(env, policy, Q, discount_factor)\n",
    "\n",
    "        # policy = update_policy_regularized_softmax(env, policy, V, temperature)\n",
    "        \n",
    "        # if policy not changed over 10 iterations it converged.\n",
    "        if i % 10 == 0:\n",
    "            if (np.all(np.equal(policy, policy_prev))):\n",
    "                print('policy converged at iteration %d' %(i+1))\n",
    "                break\n",
    "            policy_prev = np.copy(policy)\n",
    "            \n",
    "\n",
    "            \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy converged at iteration 31\n",
      "Time to converge:  94.8 ms\n",
      "Optimal Value function: \n",
      "[[0.35417188 0.33610043 0.33610043 0.32322524]\n",
      " [0.20945968 0.20219837 0.18412692 0.29789248]\n",
      " [0.27323702 0.2647293  0.257468   0.26405201]\n",
      " [0.1683579  0.1683579  0.15985018 0.24828299]\n",
      " [0.39178738 0.27802181 0.26514662 0.24040632]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27180699 0.18337418 0.27180699 0.08843281]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27802181 0.33739047 0.31265016 0.46403122]\n",
      " [0.38170735 0.56508154 0.41370048 0.33475524]\n",
      " [0.55525204 0.46614194 0.36924263 0.27511951]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.4163357  0.51045882 0.69646823 0.46614194]\n",
      " [0.69383301 0.84379216 0.79684005 0.74703381]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Final Policy: \n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Most likely actions taken by policy: \n",
      "← ↑ ← ↑ ← ← ← ← ↑ ↓ ← ← ← → ↓ ←\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackkert/anaconda3/envs/ics/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "environment2 = gym.make('FrozenLake-v1') #, render_mode = \"human\")\n",
    "tic = time.time()\n",
    "opt_V2, opt_policy2 = policy_iteration(environment2.env, discount_factor = 0.999, max_iteration = 10000)\n",
    "toc = time.time()\n",
    "elapsed_time = (toc - tic) * 1000\n",
    "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
    "print('Optimal Value function: ')\n",
    "print(opt_V2.reshape((environment2.observation_space.n, environment2.action_space.n)))\n",
    "print('Final Policy: ')\n",
    "print(np.round(opt_policy2,2))\n",
    "# Display the most likely policy in discrete action\n",
    "print(\"Most likely actions taken by policy: \")\n",
    "print(' '.join([action_mapping[(np.argmax(action))] for action in opt_policy2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 10\n",
    "wins, total_reward, avg_reward = play_episodes(environment2, n_episode, opt_policy2, random = False, categorial = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wins with Policy iteration: 9\n",
      "Average rewards with Policy iteration: 0.9\n"
     ]
    }
   ],
   "source": [
    "print(f'Total wins with Policy iteration: {wins}')\n",
    "print(f\"Average rewards with Policy iteration: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration converge faster but takes more computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
